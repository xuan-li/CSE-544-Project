{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import threading\n",
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "start_time = 1190146243\n",
    "end_time = 1192994591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_data(data, output_name):\n",
    "    with open(output_name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_data(input_name):\n",
    "    with open(input_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump word frequencies (Only English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_distribution(file_name):\n",
    "    lemmas=[]\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            url=line.strip().split(' ')[2]\n",
    "            if url[7:9] != 'en':\n",
    "                continue\n",
    "            lemma=url[url.rfind('/')+1:]\n",
    "            if ':' in lemma:\n",
    "                continue\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def count_lemma(dictionary, lemmas):\n",
    "    for lemma in lemmas:\n",
    "        if lemma not in dictionary:\n",
    "            dictionary[lemma] = 1\n",
    "        else:\n",
    "            dictionary[lemma] += 1\n",
    "\n",
    "            \n",
    "def get_lemma_counter(num, nworkers = 4):\n",
    "    data_root = \"./data/only_lemma/\"\n",
    "    data_files = os.listdir(data_root)\n",
    "    counter = {}\n",
    "\n",
    "    threads = [None] * nworkers\n",
    "    flags = [True] * nworkers\n",
    "    \n",
    "    if num > len(data_files):\n",
    "        num = len(data_files)\n",
    "    indices = np.random.choice(len(data_files), num)\n",
    "    \n",
    "    def target(index, flags, counter, filename):\n",
    "        count_lemma(counter, get_lemma_distribution(filename))\n",
    "        flags[index] = True\n",
    "        \n",
    "    j = 1\n",
    "    \n",
    "    for i in indices:\n",
    "        index = 0\n",
    "        while True:\n",
    "            try:\n",
    "                index = flags.index(True)\n",
    "                flags[index] = False\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        print(\"Processing file {}\".format(j))\n",
    "        j += 1\n",
    "        t = threading.Thread(target = target, args = [index, flags, counter, data_root + data_files[i]])\n",
    "        threads[index] = t\n",
    "        t.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        if thread:\n",
    "            thread.join()\n",
    "    return counter\n",
    "\n",
    "def word_freq_dataframe(counter, num = 2000):\n",
    "    sorted_dict = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    data = []\n",
    "    index = []\n",
    "    for i in range(num):\n",
    "        item = sorted_dict[i]\n",
    "        if len(item[0]) == 0:\n",
    "            continue\n",
    "        index.append(item[0])\n",
    "        data.append(int(item[1]))\n",
    "    return pd.Series(data= data, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = get_lemma_counter(1000, nworkers = 50)\n",
    "#dump_data(counter,\"data/dump/hot_words_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq_series = word_freq_dataframe(counter, 100000)\n",
    "#dump_data(word_freq_series, \"data/dump/hot_words_100000.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump access number per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress \n",
    "\n",
    "def get_time_distribution(file_name):\n",
    "    times=[]\n",
    "    for line in open(file_name):\n",
    "        t=float(line.strip().split(' ')[1])\n",
    "        times.append(t)\n",
    "    return times\n",
    "\n",
    "def count_timestamp(dictionary, times):\n",
    "    for time in times:\n",
    "        time = int(time)\n",
    "        if time not in dictionary:\n",
    "            dictionary[time] = 1\n",
    "        else:\n",
    "            dictionary[time] += 1\n",
    "\n",
    "def get_timestamp_counter(num, nworkers = 4):\n",
    "    data_root = \"./data/only_lemma/\"\n",
    "    data_files = os.listdir(data_root)\n",
    "    counter = {}\n",
    "\n",
    "    threads = [None] * nworkers\n",
    "    flags = [True] * nworkers\n",
    "    \n",
    "    if num > len(data_files):\n",
    "        num = len(data_files)\n",
    "    indices = np.random.choice(len(data_files), num)\n",
    "    \n",
    "    def target(index, flags, counter, filename):\n",
    "        count_timestamp(counter, get_time_distribution(filename))\n",
    "        flags[index] = True\n",
    "        \n",
    "    j = 1\n",
    "    p = IntProgress(max = len(indices))\n",
    "    display(p)\n",
    "    for i in indices:\n",
    "        index = 0\n",
    "        while True:\n",
    "            try:\n",
    "                index = flags.index(True)\n",
    "                flags[index] = False\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        #print(\"Processing file {}\".format(j))\n",
    "        j += 1\n",
    "        p.value += 1\n",
    "        t = threading.Thread(target = target, args = [index, flags, counter, data_root + data_files[i]])\n",
    "        threads[index] = t\n",
    "        t.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        if thread:\n",
    "            thread.join()\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f2cd2c11044a318d142f306c61d0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=792)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_counter = get_timestamp_counter(1000)\n",
    "dump_data(time_counter, \"data/dump/access_per_second.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump timestamps of single words (Only English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_timestamps_from_file(file_name, entry_list, timestamps):\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            timestamp, url=line.strip().split(' ')[1:3]\n",
    "            if url[7:9] != 'en':\n",
    "                continue\n",
    "            lemma=url[url.rfind('/')+1:]\n",
    "            if lemma in entry_list:\n",
    "                timestamps[lemma].append(timestamp)\n",
    "        return timestamps\n",
    "\n",
    "def get_lemma_timestamps_from_file_reset(file_name, entry_list):\n",
    "    timestamps = {}\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            timestamp, url=line.strip().split(' ')[1:3]\n",
    "            if url[7:9] != 'en':\n",
    "                continue\n",
    "            lemma=url[url.rfind('/')+1:]\n",
    "            if lemma in entry_list:\n",
    "                try:\n",
    "                    timestamps[lemma].append(timestamp)\n",
    "                except:\n",
    "                    timestamps[lemma] = [timestamp]\n",
    "        return timestamps\n",
    "\n",
    "def merge_timestamps_to_file(timestamps, output_dir):\n",
    "    for word in timestamps:\n",
    "        with open(output_dir+\"{}.csv\".format(word), 'a') as f:\n",
    "            for timestamp in timestamps[word]:\n",
    "                f.write(\"{}\\n\".format(timestamp))\n",
    "    print(\"done!\")\n",
    "\n",
    "def dump_timestamps_to_file(entry_list, output_dir, nworkers = 4):\n",
    "    data_root = \"./data/only_lemma/\"\n",
    "    data_files = os.listdir(data_root)\n",
    "    \n",
    "    flags = [True] * nworkers\n",
    "    threads = [None] * nworkers\n",
    "    \n",
    "    def target(index, flags, file_name, entry_list):\n",
    "        timestamps = get_lemma_timestamps_from_file_reset(file_name, entry_list)\n",
    "        dump_data(timestamps, output_dir + \"{}_timestamp.pkl\".format(file_name.split(\"/\")[-1])) \n",
    "        #merge_timestamps_to_file(timestamps, output_dir)\n",
    "        flags[index] = True\n",
    "    \n",
    "    i = 0\n",
    "    index = 0\n",
    "    f = open('log.txt', 'w')\n",
    "    \n",
    "    for file in data_files:\n",
    "        while True:\n",
    "            try:\n",
    "                index = flags.index(True)\n",
    "                flags[index] = False\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        i += 1\n",
    "        #print(\"Processing file {}\".format(i))\n",
    "        #target(timestamps, entry, data_root + file)\n",
    "        t = threading.Thread(target = target, args = [index, flags, data_root + file, entry_list])\n",
    "        threads[index] = t\n",
    "        t.start()\n",
    "        f.write(\"Processing file {}\\n\".format(i))\n",
    "        f.flush()\n",
    "        \n",
    "    for thread in threads:\n",
    "        if thread:\n",
    "            thread.join()\n",
    "    return timestamps\n",
    "\n",
    "def get_lemma_timestamps(entry_list, nworkers = 4):\n",
    "    timestamps = {}\n",
    "    for entry in entry_list:\n",
    "        timestamps[entry] = []\n",
    "    data_root = \"./data/only_lemma/\"\n",
    "    data_files = os.listdir(data_root)\n",
    "    \n",
    "    flags = [True] * nworkers\n",
    "    threads = [None] * nworkers\n",
    "    \n",
    "    def target(index, flags, file_name, entry_list, timestamps):\n",
    "        get_lemma_timestamps_from_file(file_name, entry_list, timestamps)\n",
    "        flags[index] = True\n",
    "    \n",
    "    i = 0\n",
    "    index = 0\n",
    "    f = open('log.txt', 'w')\n",
    "    for file in data_files:\n",
    "        while True:\n",
    "            try:\n",
    "                index = flags.index(True)\n",
    "                flags[index] = False\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        i += 1\n",
    "        #print(\"Processing file {}\".format(i))\n",
    "        #target(timestamps, entry, data_root + file)\n",
    "        t = threading.Thread(target = target, args = [index, flags, data_root + file, entry_list, timestamps])\n",
    "        threads[index] = t\n",
    "        t.start()\n",
    "        f.write(\"Processing file {}\\n\".format(i))\n",
    "        f.flush()\n",
    "        \n",
    "    for thread in threads:\n",
    "        if thread:\n",
    "            thread.join()\n",
    "    return timestamps\n",
    "    \n",
    "def get_lemma_timestamps_from_seperate(entry_list, nworkers = 4):\n",
    "    timestamps = {}\n",
    "    for entry in entry_list:\n",
    "        timestamps[entry] = []\n",
    "    data_root = \"./data/timestamps/dump/\"\n",
    "    data_files = os.listdir(data_root)\n",
    "    \n",
    "    flags = [True] * nworkers\n",
    "    threads = [None] * nworkers\n",
    "    \n",
    "    def target(index, flags, file_name, entry_list):\n",
    "        timestamps_of_file = load_data(file_name)\n",
    "        for word in timestamps:\n",
    "            try:\n",
    "                timestamps[word] += timestamps_of_file[word]\n",
    "            except:\n",
    "                pass\n",
    "        flags[index] = True\n",
    "    \n",
    "    i = 0\n",
    "    index = 0\n",
    "    for file in data_files:\n",
    "        while True:\n",
    "            try:\n",
    "                index = flags.index(True)\n",
    "                flags[index] = False\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        i += 1\n",
    "        t = threading.Thread(target = target, args = [index, flags, data_root + file, entry_list])\n",
    "        threads[index] = t\n",
    "        t.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        if thread:\n",
    "            thread.join()\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dump/word_access_timestamps_0-5000.pkl\n",
      "data/dump/word_access_timestamps_5000-10000.pkl\n",
      "data/dump/word_access_timestamps_10000-15000.pkl\n",
      "data/dump/word_access_timestamps_15000-20000.pkl\n",
      "data/dump/word_access_timestamps_20000-25000.pkl\n",
      "data/dump/word_access_timestamps_25000-30000.pkl\n",
      "data/dump/word_access_timestamps_30000-35000.pkl\n",
      "data/dump/word_access_timestamps_35000-40000.pkl\n",
      "data/dump/word_access_timestamps_40000-45000.pkl\n",
      "data/dump/word_access_timestamps_45000-50000.pkl\n",
      "data/dump/word_access_timestamps_50000-55000.pkl\n",
      "data/dump/word_access_timestamps_55000-60000.pkl\n"
     ]
    }
   ],
   "source": [
    "word_freq_series = load_data(\"data/dump/hot_words_100000.pkl\")\n",
    "delta = 5000\n",
    "for j in range(20):\n",
    "    print(\"data/dump/word_access_timestamps_{}-{}.pkl\".format(delta * j, delta * (j+1)))\n",
    "    timestamps = get_lemma_timestamps_from_seperate(word_freq_series.index[delta * j: delta * (j+1)])\n",
    "    dump_data(timestamps, \"data/dump/word_access_timestamps_{}-{}.pkl\".format(delta * j, delta * (j+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq_series = load_data(\"data/dump/hot_words_100000.pkl\")\n",
    "#dump_timestamps_to_file(word_freq_series, \"data/timestamps/dump/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = load_data('data/dump/word_access_timestamps_0-5000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(times.keys())\n",
    "seperate_set = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    seperate_set = {}\n",
    "    for word in word_list[i*1000:(i+1)*1000]:\n",
    "        seperate_set[word] = times[word]\n",
    "    dump_data(seperate_set, \"data/dump/word_access_timestamps_{}-{}.pkl\".format(i*1000,(i+1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
